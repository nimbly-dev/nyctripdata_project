### Tripdata Tables

When using Docker Compose, partitioned **Tripdata** tables will be created without data. A series of SQL queries will be executed, and you can review these queries in the deployment folder. The project includes three types of datasets:

| Dataset         | Description                               | Data Dictionary                                                   |
|-----------------|-------------------------------------------|-------------------------------------------------------------------|
| **Yellow Tripdata**  | Data containing trip records for Yellow Cab taxis. | [Yellow Tripdata Dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf) |
| **Green Tripdata**   | Data containing trip records for Green Cab taxis.  | [Green Tripdata Dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf)  |
| **FHV Tripdata**     | Data containing trip records for For-Hire Vehicles. | [FHV Tripdata Dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_fhv.pdf)    |

#### `dwid` Column

The `dwid` column is added at the stage level and is used as a primary key in various Trip datasets. It is generated by hashing a concatenation of common columns (`pickup_datetime`, `dropoff_datetime`, `pu_location_id`, and `do_location_id`) using the SHA-256 algorithm. 

Key Points:
- **Uniqueness**: The `dwid` is designed to be unique and deterministic. It is not random but derived from consistent values in the common columns.
- **Deterministic Nature**: This ensures that the `dwid` remains consistent even if the data pipeline is run multiple times. This deterministic approach supports reliable upserts (updates and inserts) without duplication.

By generating a `dwid` based on these key columns, we ensure consistent and unique identification across datasets, this also allows us to do an **upsert** operation whenever needed using this column.

#### Partitioning in the Tripdata Tables

When data is appended to Tripdata tables, the pipeline creates a partition based on the month extracted from `pickup_datetime`. Each partition corresponds to a specific month. Data is inserted into the appropriate partition after creation.

A SQL function available in all public schemas automates this process. It takes the table name and target date in datetime format, creating a partition named: {tripdata_table_name}{environment}{year}_{month}. The default partition key is `pickup_datetime`.


### Modifying the Pipeline

When modifying the pipeline, consider the following guidelines:

- **Development Stage**: Apply specific changes such as column transformations, new cleaning rules, or additional data to the development pipeline (e.g., `spark_yellow_taxi_etl_to_dev_partition`). This stage is intended for testing changes specific to a Tripdata type.

- **Stage Level**: Use this stage for ensuring data integrity, adding columns present across datasets, and pre-production data cleaning and transformation. It is suitable for global changes that affect all Tripdata types.

- **Production Level**: Make minimal changes here. This stage combines lakehouse data and stage data to maintain data integrity. Include only dynamic cleaning operations. Avoid column transformations at this level; these should be handled in the Stage level for global changes or the Development stage for specific changes.

### Monitoring Pipelines

You can monitor the progress of your pipeline directly in the Mage app. Navigate to [http://localhost:6789/pipelines](http://localhost:6789/pipelines) to view the pipeline status.

If the pipeline fails, you can investigate the issue by selecting the pipeline detail and reviewing the logs. You can also re-run the pipeline if necessary.

![Pipeline Run List](images/documentation/pipeline_run_list.JPG)

Clicking on a specific pipeline will display the individual code blocks that are executed as part of the pipeline:

![Pipeline Run Detail](images/documentation/pipeline_run_detail.JPG)

To view real-time logs or troubleshoot errors, click the logs icon, which provides a live tail of the pipelineâ€™s execution:

![Pipeline Run Log Tail](images/documentation/pipeline_log_tail.JPG)
